{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27059bd-80ef-4ced-9523-4f0668e42ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1c7ad7-38b8-4157-9ede-d7f575869629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krusl\\anaconda3\\envs\\project2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision import transforms as T\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
    "import copy\n",
    "import math\n",
    "from PIL import ImageTk, Image\n",
    "import cv2\n",
    "import albumentations as A  # our data augmentation library\n",
    "\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, Frame, Button, Canvas, Label\n",
    "from tkinter.filedialog import askopenfilename\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a458c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krusl\\AppData\\Local\\Temp\\ipykernel_11444\\2474870303.py:5: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  resized_imagg = imagg.resize((1230, 820), Image.ANTIALIAS)\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\krusl\\anaconda3\\envs\\project2\\lib\\site-packages\\PIL\\Image.py\", line 3096, in open\n",
      "    fp.seek(0)\n",
      "AttributeError: 'str' object has no attribute 'seek'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\krusl\\anaconda3\\envs\\project2\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\krusl\\AppData\\Local\\Temp\\ipykernel_11444\\2474870303.py\", line 12, in b1\n",
      "    img = resizer(filename)\n",
      "  File \"C:\\Users\\krusl\\AppData\\Local\\Temp\\ipykernel_11444\\2474870303.py\", line 4, in resizer\n",
      "    imagg = Image.open(url)\n",
      "  File \"C:\\Users\\krusl\\anaconda3\\envs\\project2\\lib\\site-packages\\PIL\\Image.py\", line 3098, in open\n",
      "    fp = io.BytesIO(fp.read())\n",
      "AttributeError: 'str' object has no attribute 'read'\n"
     ]
    }
   ],
   "source": [
    "path = \"\"\n",
    "\n",
    "def resizer(url):\n",
    "    imagg = Image.open(url)\n",
    "    resized_imagg = imagg.resize((1230, 820), Image.ANTIALIAS)\n",
    "    imagg = ImageTk.PhotoImage(resized_imagg)\n",
    "    return imagg\n",
    "\n",
    "def b1():\n",
    "    panel.delete(\"all\")\n",
    "    filename = askopenfilename()\n",
    "    img = resizer(filename)\n",
    "    panel.create_image(615, 410, anchor=\"c\", image=img)\n",
    "    panel.image = img\n",
    "\n",
    "#объявление объектов\n",
    "models = [\"FasterRCNN\", \"YOLO\"]\n",
    "window = tk.Tk()\n",
    "window.title(\"Добро пожаловать\")\n",
    "window.minsize(1400, 900)\n",
    "window.config(bg=\"skyblue\")\n",
    "\n",
    "left_frame = Frame(window, width=150, height= 820)\n",
    "right_frame = Frame(window, width=1250, height=820)\n",
    "right_frame.grid(row=0, column=1, padx=10, pady=5)\n",
    "left_frame.grid(row=0, column=0, padx=10, pady=5)\n",
    "\n",
    "\n",
    "btn1 = Button(left_frame, text='Выбрать картинку', command=b1)\n",
    "btn1.grid(row=2, column=0, padx=20, pady=20)\n",
    "btn2 = Button(left_frame, text= 'Запустить')\n",
    "btn2.grid(row=4, column=0, padx=20, pady=20)\n",
    "l = Label(left_frame, text = \"Выбор модели\")\n",
    "l.grid(row=0, column=0, padx=20, pady=20)\n",
    "combobox = ttk.Combobox(left_frame, values=models)\n",
    "combobox.grid(row=1, column=0, padx=20, pady=20)\n",
    "\n",
    "panel = Canvas(right_frame, width = 1230, height=820)\n",
    "panel.pack(fill=\"both\", expand=True)\n",
    "img = resizer(\"C:/Users/krusl/Downloads/FasterRCNN2/gray.jpg\")\n",
    "panel.create_image(615, 410, anchor=\"c\", image=img)\n",
    "panel.grid()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001aa43-ddbc-430a-916c-ddcdca5be77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # progress bar\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "!pip install pycocotools\n",
    "from pycocotools.coco import COCO\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d307931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee473a-cd96-4e62-bdca-52a285aab1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train=False):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(270, 480), # our input size can be 600px\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.ColorJitter(p=0.1),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(270, 480), # our input size can be 600px\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bcb981-abbb-4801-8b0e-0e836e40d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AquariumDetection(datasets.VisionDataset):\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
    "        # the 3 transform parameters are reuqired for datasets.VisionDataset\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.split = split #train, valid, test\n",
    "        self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\")) # annotatiosn stored here\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "    \n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "        \n",
    "        boxes = [t['bbox'] + [t['category_id']-1] for t in target] # required annotation format for albumentations\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes)\n",
    "        \n",
    "        image = transformed['image']\n",
    "        boxes = transformed['bboxes']\n",
    "        \n",
    "        new_boxes = [] # convert from xywh to xyxy\n",
    "        for box in boxes:\n",
    "            xmin = box[0]\n",
    "            xmax = xmin + box[2]\n",
    "            ymin = box[1]\n",
    "            ymax = ymin + box[3]\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        boxes = torch.as_tensor(new_boxes, dtype=torch.float32)\n",
    "        \n",
    "        tempList = [t['image_id'] for t in target]\n",
    "        tempList2 = []\n",
    "        tempList2.append(tempList[0])\n",
    "        targ = {} # here is our transformed target\n",
    "        targ['boxes'] = boxes\n",
    "        targ['labels'] = torch.as_tensor([t['category_id']-1 for t in target], dtype=torch.int64)\n",
    "        targ['image_id'] = torch.tensor(tempList2)\n",
    "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # we have a different area\n",
    "        targ['iscrowd'] = torch.as_tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "        return image.div(255), targ # scale images\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd3d50-be64-4d2b-bd88-9980b9dd5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:/Users/krusl/Downloads/FasterRCNN2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd25b84-3423-47e0-9963-51cec44730e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = COCO(os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"))\n",
    "categories = coco.cats\n",
    "n_classes = len(categories.keys())\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e06031-3a67-4d37-8f73-67d9501bb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [i[1]['name'] for i in categories.items()]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21fba6-e5ae-4ef5-8746-b2ba44f86431",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AquariumDetection(root=dataset_path, transforms=get_transforms(True))\n",
    "valid_dataset = AquariumDetection(root=dataset_path, split='valid', transforms=get_transforms(True))\n",
    "#train_dataset._load_target(1)\n",
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c5c31-526a-49aa-94c0-23699682bbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21055a55-ff54-44db-914a-e397c3c8bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset._load_target(22))\n",
    "sample = train_dataset[2]\n",
    "img_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\n",
    "plt.imshow(draw_bounding_boxes(\n",
    "    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n",
    ").permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbec03-122d-4433-b783-34fc09d6748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features # we need to change the head\n",
    "print(n_classes)\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b8cc6-9083-4917-865c-a3f1a46be6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205144f-403b-4e9d-a2bb-cb695eb5d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=6, num_workers = 0,shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=6, num_workers = 0,shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489476a-e0de-4af8-9081-f652041acd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,targets = next(iter(train_loader))\n",
    "\n",
    "images = list(image for image in images)\n",
    "targets = [{k:v for k, v in t.items()} for t in targets]\n",
    "print(len(targets))\n",
    "output = model(images, targets) # just make sure this runs without error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a5fcd-77e6-4091-b7d0-99be9de71b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") # use GPU to train\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a16d9-897d-46cd-bda8-dc6bd8a72e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437645e-1047-47b2-8f15-97aeb306554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f381e51d-4ea0-4f5f-b9d5-9703d08e0206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_one_epoch(model, optimizer, loader, device, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "#     lr_scheduler = None\n",
    "#     if epoch == 0:\n",
    "#         warmup_factor = 1.0 / 1000 # do lr warmup\n",
    "#         warmup_iters = min(1000, len(loader) - 1)\n",
    "        \n",
    "#         lr_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor = warmup_factor, total_iters=warmup_iters)\n",
    "    \n",
    "    all_losses = []\n",
    "    all_losses_dict = []\n",
    "    \n",
    "    for images, targets in tqdm(loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets) # the model computes the loss automatically if we pass in targets\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        all_losses.append(loss_value)\n",
    "        all_losses_dict.append(loss_dict_append)\n",
    "        \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if lr_scheduler is not None:\n",
    "#             lr_scheduler.step() # \n",
    "        \n",
    "    all_losses_dict = pd.DataFrame(all_losses_dict) # for printing\n",
    "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
    "        all_losses_dict['loss_classifier'].mean(),\n",
    "        all_losses_dict['loss_box_reg'].mean(),\n",
    "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
    "        all_losses_dict['loss_objectness'].mean()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbb58d-7307-479c-8254-4ca9c8267e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=1\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=1000)\n",
    "    evaluate(model, valid_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e44f0-d2bf-489b-87bd-c91b36ee544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.save(model.state_dict(), \"C:/Users/krusl/Downloads/FasterRCNN2/model.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9efcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d096a8a-62be-49a3-947f-775584f880a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'C:/Users/krusl/Downloads/FasterRCNN2/test/trash2.jpg'\n",
    "from torchvision.io import read_image\n",
    "pic = read_image(path)\n",
    "img_int = torch.tensor(pic, dtype=torch.uint8)\n",
    "with torch.no_grad():\n",
    "    prediction = model([pic.div(255).to(device)])\n",
    "    pred = prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898c7d4-b765-438a-84b3-9178d1dc30f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc4c1c-817a-416e-82d3-5d5534cbf51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 10))\n",
    "plt.imshow(draw_bounding_boxes(img_int,\n",
    "    pred['boxes'][pred['scores'] > 0.3],\n",
    "    [classes[i] for i in pred['labels'][pred['scores'] > 0.3].tolist()], width=4\n",
    ").permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36358468-dd71-4e0a-963d-78117462fc73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c10e5-1712-4b49-98c1-249e6689f2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
